{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # 이름을 명시해서 특정 자원만 설치할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', 'it', 'possible', 'distinguishing', 'cats', 'and', 'dogs', '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"Is it possible distinguishing cats and dogs?\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('possible', 'JJ'),\n",
       " ('distinguishing', 'VBG'),\n",
       " ('cats', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('dogs', 'NNS'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한국어도', '할', '수', '있겠니', '?', '궁금하다', '야']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kotxt = nltk.word_tokenize(\"한국어도 할 수 있겠니? 궁금하다 야\")\n",
    "kotxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'favorite', 'subject', 'is', 'math']\n",
      "['my', 'favorite', 'subject', 'is', 'math', ',', 'english', ',', 'economic', 'and', 'computer', 'science']\n"
     ]
    }
   ],
   "source": [
    "str1 = \"my favorite subject is math\"\n",
    "str2 = \"my favorite subject is math, english, economic and computer science\"\n",
    "\n",
    "print(nltk.word_tokenize(str1))\n",
    "print(nltk.word_tokenize(str2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 세그멘테이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A few days ago, we published another blog post related to Next.js: Understanding Next.js Data Fetching for Beginners.',\n",
       " 'In that blog post, we show the functions this framework provides us to perform different data fetching options.',\n",
       " 'I hope you already have a basic understanding of Next.js.',\n",
       " 'If not, I highly recommend that you first read the article linked above.',\n",
       " \"Let's get started.\",\n",
       " 'There are multiple strategies for us to choose from, and it gets confusing at first to apply the correct solution in the correct place.',\n",
       " 'We will take a real-life application, and understand which strategy works best for which scenarios.',\n",
       " 'The special functions that are used for pre-rendering in Next.js are:\\n\\ngetStaticProps\\ngetStaticPaths\\ngetServerSideProps\\nWe will see in which scenarios you can apply these functions, and learn the best practices for doing so.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sample = \"\"\"A few days ago, we published another blog post related to Next.js: Understanding Next.js Data Fetching for Beginners.\n",
    "\n",
    "In that blog post, we show the functions this framework provides us to perform different data fetching options. I hope you already have a basic understanding of Next.js. If not, I highly recommend that you first read the article linked above.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "There are multiple strategies for us to choose from, and it gets confusing at first to apply the correct solution in the correct place. We will take a real-life application, and understand which strategy works best for which scenarios.\n",
    "\n",
    "The special functions that are used for pre-rendering in Next.js are:\n",
    "\n",
    "getStaticProps\n",
    "getStaticPaths\n",
    "getServerSideProps\n",
    "We will see in which scenarios you can apply these functions, and learn the best practices for doing so.\"\"\"\n",
    "\n",
    "tokenized = sent_tokenize(sample)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " \"'s\",\n",
       " 'nothing',\n",
       " 'that',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'already',\n",
       " 'know',\n",
       " 'blabla']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"it's nothing that you don't already know blabla\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "words # ['it', \"'s\", 'nothing', 'that', 'you', 'don', \"'t\", 'already', 'know', 'blabla']\n",
    "# 생각해보면 it's -> it 's 두 개로 분리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'nothing',\n",
       " 'that',\n",
       " 'you',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'already',\n",
       " 'know',\n",
       " 'blabla']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(sentence) # '를 별개의 단어 취급하여 it's -> it ' s 의 3개로 분리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "054d12451c88fa6cddb0db320e2c73f4d9b272cf1e754592ad6958aa88348a06"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
